#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script ƒë·ªÉ so s√°nh k·∫øt qu·∫£ facet gi·ªØa 3 Solr containers
- L·∫•y 100 documents ƒë·∫ßu ti√™n t·ª´ Solr
- Query t·ª´ng ID tr√™n c·∫£ 3 containers
- So s√°nh k·∫øt qu·∫£ facet gi·ªØa c√°c containers

C√°ch s·ª≠ d·ª•ng:
    python compare_facet_results.py [num_docs] [source_port]
    
Tham s·ªë:
    num_docs: S·ªë documents ƒë·ªÉ l·∫•y (m·∫∑c ƒë·ªãnh: 100)
    source_port: Port c·ªßa Solr ƒë·ªÉ l·∫•y danh s√°ch documents (m·∫∑c ƒë·ªãnh: 8983)
"""

import requests
import json
import sys
from urllib.parse import urlencode
from collections import defaultdict
import time
from datetime import datetime

# Th·ª≠ import openpyxl, n·∫øu kh√¥ng c√≥ th√¨ s·∫Ω b√°o l·ªói khi c·∫ßn
try:
    from openpyxl import Workbook
    from openpyxl.styles import Font, Alignment, PatternFill
    from openpyxl.utils import get_column_letter
    HAS_OPENPYXL = True
except ImportError:
    HAS_OPENPYXL = False

# Tham s·ªë t·ª´ command line
NUM_DOCS = int(sys.argv[1]) if len(sys.argv) > 1 else 1000
SOURCE_PORT = int(sys.argv[2]) if len(sys.argv) > 2 else 8983

# Query parameters cho facet
FACET_PARAMS = {
    "q": "*:*",
    "facet": "true",
    "facet.field": "search_text_cloud",
    "facet.sort": "count",
    "rows": "0",
    "wt": "json",
    "indent": "true",
    "facet.limit": "1000",
    "facet.mincount": "1"
}

# Container configurations
CONTAINERS = [
    {
        "name": "solr_8_5_2_1_1",
        "port": 8983,
        "core": "topic_tanvd",
        "version": "Solr 8.5.2 (VnCoreNLP 1.1.1)"
    },
    {
        "name": "solr_8_5_2_1_2",
        "port": 8984,
        "core": "topic_tanvd",
        "version": "Solr 8.5.2 (VnCoreNLP 1.2)"
    },
    {
        "name": "solr_9_11",
        "port": 8985,
        "core": "topic_tanvd_9",
        "version": "Solr 9.11"
    }
]


def get_document_ids(port, core, num_docs):
    """L·∫•y danh s√°ch ID t·ª´ Solr"""
    url = f"http://localhost:{port}/solr/{core}/select"
    params = {
        "q": "*:*",
        "rows": num_docs,
        "fl": "id",
        "wt": "json"
    }
    
    try:
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        ids = [doc["id"] for doc in data.get("response", {}).get("docs", [])]
        return ids
    except Exception as e:
        print(f"‚ùå ERROR khi l·∫•y document IDs: {str(e)}")
        return []


def get_search_text(port, core, doc_id):
    """L·∫•y field search_text t·ª´ document"""
    url = f"http://localhost:{port}/solr/{core}/select"
    params = {
        "q": f"id:{doc_id}",
        "fl": "search_text",
        "rows": 1,
        "wt": "json"
    }
    
    try:
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        docs = data.get("response", {}).get("docs", [])
        if docs and "search_text" in docs[0]:
            search_text = docs[0]["search_text"]
            # N·∫øu l√† list, join l·∫°i th√†nh string
            if isinstance(search_text, list):
                return "\n".join(str(item) for item in search_text)
            return str(search_text)
        return ""
    except Exception as e:
        return ""


def get_facet_results(port, core, doc_id):
    """L·∫•y k·∫øt qu·∫£ facet cho m·ªôt document ID"""
    url = f"http://localhost:{port}/solr/{core}/select"
    params = FACET_PARAMS.copy()
    params["fq"] = f"id:{doc_id}"
    
    try:
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        # Extract facet terms
        facet_field = data.get("facet_counts", {}).get("facet_fields", {}).get("search_text_cloud", [])
        
        # Convert t·ª´ array [term1, count1, term2, count2, ...] th√†nh dict
        facet_dict = {}
        for i in range(0, len(facet_field), 2):
            if i + 1 < len(facet_field):
                term = facet_field[i]
                count = facet_field[i + 1]
                facet_dict[term] = count
        
        return facet_dict, data.get("response", {}).get("numFound", 0)
    except Exception as e:
        print(f"   ‚ùå ERROR khi query ID {doc_id}: {str(e)}")
        return {}, 0


def compare_facet_results(results_dict):
    """So s√°nh k·∫øt qu·∫£ facet gi·ªØa c√°c containers"""
    comparisons = []
    
    # So s√°nh t·ª´ng c·∫∑p containers
    for i in range(len(CONTAINERS)):
        for j in range(i + 1, len(CONTAINERS)):
            container1 = CONTAINERS[i]
            container2 = CONTAINERS[j]
            
            version1 = container1["version"]
            version2 = container2["version"]
            
            # ƒê·∫øm s·ªë documents c√≥ k·∫øt qu·∫£ gi·ªëng nhau v√† kh√°c nhau
            same_count = 0
            diff_count = 0
            only_in_1 = 0
            only_in_2 = 0
            total_terms_diff = 0
            
            for doc_id in results_dict:
                facets1 = results_dict[doc_id].get(version1, {})
                facets2 = results_dict[doc_id].get(version2, {})
                
                if facets1 == facets2:
                    same_count += 1
                else:
                    diff_count += 1
                    
                    # T√≠nh to√°n s·ª± kh√°c bi·ªát
                    terms1 = set(facets1.keys())
                    terms2 = set(facets2.keys())
                    
                    only_in_1_count = len(terms1 - terms2)
                    only_in_2_count = len(terms2 - terms1)
                    
                    if only_in_1_count > 0:
                        only_in_1 += 1
                    if only_in_2_count > 0:
                        only_in_2 += 1
                    
                    total_terms_diff += abs(len(terms1) - len(terms2))
            
            comparisons.append({
                "container1": version1,
                "container2": version2,
                "same": same_count,
                "different": diff_count,
                "only_in_1": only_in_1,
                "only_in_2": only_in_2,
                "avg_terms_diff": total_terms_diff / diff_count if diff_count > 0 else 0
            })
    
    return comparisons


def export_to_excel(results_dict, search_text_dict, ids, timestamp, logger):
    """Xu·∫•t k·∫øt qu·∫£ facet ra file Excel"""
    if not HAS_OPENPYXL:
        logger.log("‚ö†Ô∏è  Th∆∞ vi·ªán openpyxl ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Kh√¥ng th·ªÉ t·∫°o file Excel.")
        logger.log("   C√†i ƒë·∫∑t b·∫±ng l·ªánh: pip install openpyxl")
        return None
    
    excel_file = f"facet_comparison_results_{timestamp}.xlsx"
    
    logger.log("‚îÅ" * 70)
    logger.log("B∆∞·ªõc 6: Xu·∫•t k·∫øt qu·∫£ ra file Excel")
    logger.log("‚îÅ" * 70)
    logger.log()
    logger.log(f"üìä ƒêang t·∫°o file Excel: {excel_file}")
    
    # T·∫°o workbook v√† worksheet
    wb = Workbook()
    ws = wb.active
    ws.title = "Facet Comparison"
    
    # ƒê·ªãnh nghƒ©a header
    headers = ["Document ID", "search_text", "Solr 8.5.2 (VnCoreNLP 1.1.1)", "Solr 8.5.2 (VnCoreNLP 1.2)", "Solr 9.11"]
    
    # Style cho header
    header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
    header_font = Font(bold=True, color="FFFFFF", size=11)
    header_alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)
    
    # Ghi header
    for col_idx, header in enumerate(headers, 1):
        cell = ws.cell(row=1, column=col_idx, value=header)
        cell.fill = header_fill
        cell.font = header_font
        cell.alignment = header_alignment
    
    # Style cho data cells
    data_alignment = Alignment(vertical="top", wrap_text=True)
    data_font = Font(size=10)
    
    # Ghi d·ªØ li·ªáu t·ª´ng document
    row_idx = 2
    for doc_id in ids:
        # C·ªôt 1: Document ID
        ws.cell(row=row_idx, column=1, value=doc_id).font = Font(bold=True, size=10)
        
        # C·ªôt 2: search_text
        search_text = search_text_dict.get(doc_id, "")
        cell_search_text = ws.cell(row=row_idx, column=2, value=search_text)
        cell_search_text.font = data_font
        cell_search_text.alignment = data_alignment
        
        # C·ªôt 3-5: Facet results cho t·ª´ng container
        for col_idx, container in enumerate(CONTAINERS, 3):
            facets = results_dict[doc_id].get(container["version"], {})
            
            if not facets:
                cell_value = "Document kh√¥ng t·ªìn t·∫°i"
                cell = ws.cell(row=row_idx, column=col_idx, value=cell_value)
                cell.font = Font(size=10, italic=True, color="808080")
            else:
                # Format facet results: term1 (count1), term2 (count2), ...
                facet_items = []
                for term, count in sorted(facets.items(), key=lambda x: (-x[1], x[0])):  # Sort by count desc, then term
                    facet_items.append(f"{term} ({count})")
                
                cell_value = "\n".join(facet_items)
                cell = ws.cell(row=row_idx, column=col_idx, value=cell_value)
                cell.font = data_font
                cell.alignment = data_alignment
            
            cell.alignment = data_alignment
        
        row_idx += 1
    
    # ƒêi·ªÅu ch·ªânh ƒë·ªô r·ªông c·ªôt
    ws.column_dimensions['A'].width = 40  # Document ID
    ws.column_dimensions['B'].width = 60  # search_text
    ws.column_dimensions['C'].width = 50  # Solr 8.5.2 (VnCoreNLP 1.1.1)
    ws.column_dimensions['D'].width = 50  # Solr 8.5.2 (VnCoreNLP 1.2)
    ws.column_dimensions['E'].width = 50  # Solr 9.11
    
    # ƒê·∫∑t chi·ªÅu cao h√†ng t·ª± ƒë·ªông
    for row in ws.iter_rows(min_row=2, max_row=row_idx):
        max_lines = 1
        for cell in row[1:]:  # B·ªè qua c·ªôt Document ID
            if cell.value:
                lines = str(cell.value).count('\n') + 1
                max_lines = max(max_lines, lines)
        ws.row_dimensions[row[0].row].height = min(max_lines * 15, 300)  # Max 300px
    
    # ƒê√≥ng bƒÉng h√†ng ƒë·∫ßu ti√™n (header) v√† c·ªôt Document ID
    ws.freeze_panes = 'B2'
    
    # T·∫°o sheet th·ªëng k√™
    stats_ws = wb.create_sheet("Statistics")
    
    # Header cho sheet Statistics
    stats_headers = ["Metric", "Value"]
    for col_idx, header in enumerate(stats_headers, 1):
        cell = stats_ws.cell(row=1, column=col_idx, value=header)
        cell.fill = header_fill
        cell.font = header_font
        cell.alignment = header_alignment
    
    # T√≠nh to√°n th·ªëng k√™
    total_docs = len(ids)
    stats_data = [
        ["T·ªïng s·ªë documents", total_docs],
        ["Solr 8.5.2 (VnCoreNLP 1.1.1)", ""],
        ["  - Documents c√≥ facet", sum(1 for doc_id in ids if results_dict[doc_id].get(CONTAINERS[0]["version"], {}))],
        ["Solr 8.5.2 (VnCoreNLP 1.2)", ""],
        ["  - Documents c√≥ facet", sum(1 for doc_id in ids if results_dict[doc_id].get(CONTAINERS[1]["version"], {}))],
        ["Solr 9.11", ""],
        ["  - Documents c√≥ facet", sum(1 for doc_id in ids if results_dict[doc_id].get(CONTAINERS[2]["version"], {}))],
    ]
    
    # T√≠nh s·ªë documents c√≥ s·ª± kh√°c bi·ªát
    same_count = 0
    diff_count = 0
    for doc_id in ids:
        facets1 = results_dict[doc_id].get(CONTAINERS[0]["version"], {})
        facets2 = results_dict[doc_id].get(CONTAINERS[1]["version"], {})
        facets3 = results_dict[doc_id].get(CONTAINERS[2]["version"], {})
        
        if facets1 == facets2 == facets3:
            same_count += 1
        else:
            diff_count += 1
    
    stats_data.extend([
        ["", ""],
        ["So s√°nh", ""],
        ["  - Documents gi·ªëng nhau (c·∫£ 3 containers)", same_count],
        ["  - Documents kh√°c nhau", diff_count],
    ])
    
    # Ghi th·ªëng k√™
    for row_idx, (metric, value) in enumerate(stats_data, 2):
        stats_ws.cell(row=row_idx, column=1, value=metric).font = data_font
        stats_ws.cell(row=row_idx, column=2, value=value).font = data_font
    
    # ƒêi·ªÅu ch·ªânh ƒë·ªô r·ªông c·ªôt cho sheet Statistics
    stats_ws.column_dimensions['A'].width = 50
    stats_ws.column_dimensions['B'].width = 20
    
    # L∆∞u file
    wb.save(excel_file)
    logger.log(f"‚úÖ ƒê√£ t·∫°o file Excel: {excel_file}")
    logger.log()
    
    return excel_file


class Logger:
    """Class ƒë·ªÉ log v·ª´a ra console v·ª´a v√†o file"""
    def __init__(self, log_file):
        self.log_file = log_file
        self.file = open(log_file, 'w', encoding='utf-8')
    
    def log(self, message='', end='\n'):
        """Ghi message v√†o c·∫£ console v√† file"""
        print(message, end=end)
        self.file.write(str(message) + (end if end == '\n' else ''))
        self.file.flush()
    
    def close(self):
        """ƒê√≥ng file"""
        self.file.close()


def main():
    # T·∫°o log file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = f"facet_comparison_log_{timestamp}.txt"
    logger = Logger(log_file)
    
    try:
        logger.log("‚îÅ" * 70)
        logger.log("üîç So s√°nh k·∫øt qu·∫£ Facet gi·ªØa 3 Solr Containers")
        logger.log("‚îÅ" * 70)
        logger.log(f"\nüìã S·ªë documents: {NUM_DOCS}")
        logger.log(f"üìã Source port: {SOURCE_PORT}")
        logger.log(f"üìù Log file: {log_file}")
        logger.log()
    
        # B∆∞·ªõc 1: L·∫•y danh s√°ch document IDs
        logger.log("‚îÅ" * 70)
        logger.log("B∆∞·ªõc 1: L·∫•y danh s√°ch document IDs")
        logger.log("‚îÅ" * 70)
        logger.log()
        
        source_container = CONTAINERS[0]  # D√πng container ƒë·∫ßu ti√™n l√†m source
        ids = get_document_ids(SOURCE_PORT, source_container["core"], NUM_DOCS)
        
        if not ids:
            logger.log("‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c document IDs. Ki·ªÉm tra l·∫°i Solr containers.")
            sys.exit(1)
        
        logger.log(f"‚úÖ ƒê√£ l·∫•y ƒë∆∞·ª£c {len(ids)} document IDs")
        logger.log(f"   V√≠ d·ª• IDs: {ids[:5] if len(ids) >= 5 else ids}")
        logger.log()
    
        # B∆∞·ªõc 2: Query t·ª´ng ID tr√™n c·∫£ 3 containers
        logger.log("‚îÅ" * 70)
        logger.log("B∆∞·ªõc 2: Query t·ª´ng ID tr√™n c·∫£ 3 containers")
        logger.log("‚îÅ" * 70)
        logger.log()
        
        results_dict = defaultdict(dict)
        search_text_dict = {}  # L∆∞u search_text cho m·ªói document
        total_queries = len(ids) * len(CONTAINERS)
        current_query = 0
        
        start_time = time.time()
        
        for idx, doc_id in enumerate(ids, 1):
            logger.log(f"üìÑ Processing document {idx}/{len(ids)}: {doc_id}")
            
            # L·∫•y search_text t·ª´ container ƒë·∫ßu ti√™n
            search_text = get_search_text(SOURCE_PORT, source_container["core"], doc_id)
            search_text_dict[doc_id] = search_text
            
            for container in CONTAINERS:
                current_query += 1
                logger.log(f"   üîç Querying {container['version']}...", end=" ")
                
                facets, num_found = get_facet_results(
                    container["port"],
                    container["core"],
                    doc_id
                )
                
                results_dict[doc_id][container["version"]] = facets
                
                if num_found == 0:
                    logger.log(f"‚ö†Ô∏è  Document kh√¥ng t·ªìn t·∫°i")
                else:
                    logger.log(f"‚úÖ {len(facets)} facet terms")
            
            # Hi·ªÉn th·ªã progress
            if idx % 10 == 0:
                elapsed = time.time() - start_time
                avg_time = elapsed / current_query
                remaining = (total_queries - current_query) * avg_time
                logger.log(f"   ‚è±Ô∏è  Progress: {current_query}/{total_queries} queries ({idx}/{len(ids)} docs)")
                logger.log(f"   ‚è±Ô∏è  Estimated time remaining: {remaining:.1f}s")
            logger.log()
        
        elapsed_time = time.time() - start_time
        logger.log(f"‚úÖ Ho√†n th√†nh query {total_queries} queries trong {elapsed_time:.2f} gi√¢y")
        logger.log()
    
        # B∆∞·ªõc 3: So s√°nh k·∫øt qu·∫£
        logger.log("‚îÅ" * 70)
        logger.log("B∆∞·ªõc 3: So s√°nh k·∫øt qu·∫£")
        logger.log("‚îÅ" * 70)
        logger.log()
        
        comparisons = compare_facet_results(results_dict)
        
        for comp in comparisons:
            logger.log(f"üìä So s√°nh: {comp['container1']} vs {comp['container2']}")
            logger.log(f"   ‚úÖ Gi·ªëng nhau: {comp['same']} documents ({comp['same']*100/len(ids):.1f}%)")
            logger.log(f"   ‚ùå Kh√°c nhau: {comp['different']} documents ({comp['different']*100/len(ids):.1f}%)")
            
            if comp['different'] > 0:
                logger.log(f"   üìà Documents ch·ªâ c√≥ trong {comp['container1']}: {comp['only_in_1']}")
                logger.log(f"   üìà Documents ch·ªâ c√≥ trong {comp['container2']}: {comp['only_in_2']}")
                logger.log(f"   üìä Trung b√¨nh s·ªë terms kh√°c nhau: {comp['avg_terms_diff']:.2f}")
            logger.log()
    
        # B∆∞·ªõc 4: T√¨m c√°c documents c√≥ s·ª± kh√°c bi·ªát l·ªõn nh·∫•t
        logger.log("‚îÅ" * 70)
        logger.log("B∆∞·ªõc 4: Documents c√≥ s·ª± kh√°c bi·ªát l·ªõn nh·∫•t")
        logger.log("‚îÅ" * 70)
        logger.log()
        
        diff_docs = []
        for doc_id in results_dict:
            facets1 = results_dict[doc_id].get(CONTAINERS[0]["version"], {})
            facets2 = results_dict[doc_id].get(CONTAINERS[1]["version"], {})
            facets3 = results_dict[doc_id].get(CONTAINERS[2]["version"], {})
            
            # T√≠nh ƒë·ªô kh√°c bi·ªát
            terms1 = set(facets1.keys())
            terms2 = set(facets2.keys())
            terms3 = set(facets3.keys())
            
            all_terms = terms1 | terms2 | terms3
            common_terms = terms1 & terms2 & terms3
            
            diff_score = len(all_terms) - len(common_terms)
            
            if diff_score > 0:
                diff_docs.append({
                    "id": doc_id,
                    "diff_score": diff_score,
                    "terms_count": {
                        CONTAINERS[0]["version"]: len(terms1),
                        CONTAINERS[1]["version"]: len(terms2),
                        CONTAINERS[2]["version"]: len(terms3)
                    },
                    "only_in": {
                        CONTAINERS[0]["version"]: list(terms1 - terms2 - terms3),
                        CONTAINERS[1]["version"]: list(terms2 - terms1 - terms3),
                        CONTAINERS[2]["version"]: list(terms3 - terms1 - terms2)
                    }
                })
        
        # S·∫Øp x·∫øp theo ƒë·ªô kh√°c bi·ªát
        diff_docs.sort(key=lambda x: x["diff_score"], reverse=True)
        
        # Hi·ªÉn th·ªã top 10 documents c√≥ s·ª± kh√°c bi·ªát l·ªõn nh·∫•t
        logger.log("Top 10 documents c√≥ s·ª± kh√°c bi·ªát l·ªõn nh·∫•t:")
        logger.log()
        for idx, doc in enumerate(diff_docs[:10], 1):
            logger.log(f"{idx}. ID: {doc['id']}")
            logger.log(f"   ƒê·ªô kh√°c bi·ªát: {doc['diff_score']} terms")
            logger.log(f"   S·ªë terms:")
            for version, count in doc['terms_count'].items():
                logger.log(f"      - {version}: {count}")
            
            # Hi·ªÉn th·ªã c√°c terms ch·ªâ c√≥ trong t·ª´ng container
            for version, terms in doc['only_in'].items():
                if terms:
                    terms_str = ', '.join(terms[:20])
                    if len(terms) > 20:
                        terms_str += f" ... (v√† {len(terms) - 20} terms kh√°c)"
                    logger.log(f"   Terms ch·ªâ c√≥ trong {version}: {terms_str}")
            logger.log()
    
        # L∆∞u k·∫øt qu·∫£ chi ti·∫øt v√†o file text
        logger.log("‚îÅ" * 70)
        logger.log("B∆∞·ªõc 5: Chi ti·∫øt k·∫øt qu·∫£ t·ª´ng document")
        logger.log("‚îÅ" * 70)
        logger.log()
        
        # Log chi ti·∫øt c√°c documents c√≥ kh√°c bi·ªát
        logger.log(f"üìã Danh s√°ch t·∫•t c·∫£ {len(diff_docs)} documents c√≥ s·ª± kh√°c bi·ªát:")
        logger.log()
        for idx, doc in enumerate(diff_docs, 1):
            logger.log(f"{idx}. Document ID: {doc['id']}")
            logger.log(f"   ƒê·ªô kh√°c bi·ªát: {doc['diff_score']} terms")
            logger.log(f"   S·ªë terms trong m·ªói container:")
            for version, count in doc['terms_count'].items():
                logger.log(f"      - {version}: {count} terms")
            
            # Log chi ti·∫øt c√°c terms ch·ªâ c√≥ trong t·ª´ng container
            for version, terms in doc['only_in'].items():
                if terms:
                    logger.log(f"   Terms ch·ªâ c√≥ trong {version} ({len(terms)} terms):")
                    # Chia th√†nh c√°c d√≤ng ƒë·ªÉ d·ªÖ ƒë·ªçc
                    for i in range(0, len(terms), 10):
                        terms_batch = terms[i:i+10]
                        logger.log(f"      {', '.join(terms_batch)}")
            logger.log()
        
        # L∆∞u k·∫øt qu·∫£ v√†o file JSON
        json_file = f"facet_comparison_results_{timestamp}.json"
        output_data = {
            "metadata": {
                "num_docs": len(ids),
                "source_port": SOURCE_PORT,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "elapsed_time": elapsed_time
            },
            "comparisons": comparisons,
            "top_differences": diff_docs[:20],  # Top 20
            "all_differences": diff_docs,  # T·∫•t c·∫£ documents c√≥ kh√°c bi·ªát
            "sample_results": {doc_id: results_dict[doc_id] for doc_id in ids[:10]}  # M·∫´u 10 documents ƒë·∫ßu
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        logger.log(f"üíæ K·∫øt qu·∫£ JSON ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {json_file}")
        logger.log()
        
        # Xu·∫•t ra Excel
        excel_file = export_to_excel(results_dict, search_text_dict, ids, timestamp, logger)
        
        # T√≥m t·∫Øt
        logger.log("‚îÅ" * 70)
        logger.log("üìä T√≥m t·∫Øt")
        logger.log("‚îÅ" * 70)
        logger.log(f"‚úÖ ƒê√£ query {len(ids)} documents tr√™n {len(CONTAINERS)} containers")
        logger.log(f"‚úÖ T·ªïng s·ªë queries: {total_queries}")
        logger.log(f"‚è±Ô∏è  Th·ªùi gian th·ª±c thi: {elapsed_time:.2f} gi√¢y")
        logger.log(f"üìà T·ªëc ƒë·ªô trung b√¨nh: {total_queries/elapsed_time:.2f} queries/gi√¢y")
        logger.log(f"üìù Log file: {log_file}")
        logger.log(f"üìÑ JSON file: {json_file}")
        if excel_file:
            logger.log(f"üìä Excel file: {excel_file}")
        logger.log()
        
    finally:
        logger.close()


if __name__ == "__main__":
    main()
